{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import pandas as pd\n",
    "import geopy\n",
    "import geopy.distance\n",
    "import numpy as np\n",
    "import shapely.vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = pd.read_csv('data/geo_df.csv')\n",
    "data = pd.read_csv('data/document_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFYING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for countries of high confidence\n",
    "geo = geo[geo['country_conf'] > 0.8]\n",
    "geo = geo[pd.notnull(geo['country_predicted'])]\n",
    "geo.lat = geo.lat.astype(float)\n",
    "geo.lon = geo.lon.astype(float)\n",
    "\n",
    "# Creating the main dataset\n",
    "data_geo = pd.merge(data,geo,left_on=\"id\",right_on=\"doc_id\")\n",
    "indexNames = data_geo[(data_geo['seen'] == 1) & (data_geo['relevant']==0)].index\n",
    "data_geo.drop(indexNames, inplace=True)\n",
    "\n",
    "columns_with_bad_names = [c for c  in data_geo.columns if \"hidden\" in c]\n",
    "columns_with_bad_names.append(\"4 - Behavioral interventions\")\n",
    "data_geo.drop(columns_with_bad_names, 1, inplace=True)\n",
    "\n",
    "data_geo.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING THE FUNCTIONS REQUIRED TO PLOT THE DATA: TAKEN FROM MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate great distance between two points on Earth\n",
    "def new_haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1[:,None]\n",
    "    dlat = lat2 - lat1[:,None]\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1[:,None]) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    \n",
    "    return km\n",
    "\n",
    "# Function to calculate the density grid for plotting\n",
    "\n",
    "def density_grid(degrees,distance,df):\n",
    "    \n",
    "    df_countries = df[df[\"feature_code\"]==\"PCLI\"]\n",
    "    df_places = df[df[\"feature_code\"]!=\"PCLI\"]\n",
    "\n",
    "    # linspace(start, stop, number of evenly spaced numbers)\n",
    "    latbins = np.linspace(-90,90, round(180/degrees))\n",
    "    lonbins = np.linspace(-180,180, round(360/degrees))\n",
    "\n",
    "    n = np.zeros((len(latbins),len(lonbins)))\n",
    "    \n",
    "    print(f\"calculating density grid of size: {n.size}\")\n",
    "\n",
    "    for i,lat in enumerate(latbins):\n",
    "        # Calculating the geodesic distance between two points\n",
    "        r = geopy.distance.distance(kilometers=distance)\n",
    "        latp = geopy.Point((lat,135))\n",
    "        # if the latitude is closer than distance to the north pole, then the northern bound should be \n",
    "        # the north pole, not distancekm north of the latitude (which will pass the pole and go south again)\n",
    "        if geopy.distance.great_circle(latp,(90,135)).km < distance:\n",
    "            r_nbound = 90   \n",
    "        else:\n",
    "            r_nbound = r.destination(point=latp,bearing=0).latitude\n",
    "        # Same as above for the south pole\n",
    "        if geopy.distance.great_circle(latp,(-90,135)).km < distance:\n",
    "            r_sbound = -90   \n",
    "        else:\n",
    "            r_sbound = r.destination(point=latp,bearing=180).latitude        \n",
    "\n",
    "        latbound_df = df_places[\n",
    "            (df_places.lat>=r_sbound) &\n",
    "            (df_places.lat<=r_nbound)        \n",
    "        ]\n",
    "\n",
    "        ds = new_haversine_np(latbound_df['lon'], latbound_df['lat'],lonbins,[lat]*len(lonbins))\n",
    "\n",
    "        n[i,:] = np.where(ds<distance,1,0).sum(axis=0)\n",
    "        \n",
    "    print(\"done\")\n",
    "    \n",
    "    shpfilename = shpreader.natural_earth(resolution='110m',\n",
    "                                          category='cultural',\n",
    "                                          name='admin_0_countries')\n",
    "    reader = shpreader.Reader(shpfilename)\n",
    "    yv, xv = np.meshgrid(latbins, lonbins)\n",
    "\n",
    "    for country in reader.records():\n",
    "        incountry = shapely.vectorized.contains(country.geometry,xv,yv)\n",
    "        idx = np.argwhere(incountry==True)\n",
    "        ndots = idx.size/2\n",
    "        cdf = df_countries[df_countries[\"country_predicted\"]==country.attributes[\"SU_A3\"]]\n",
    "        for point in idx:\n",
    "            n[point[1],point[0]] += cdf.shape[0]/ndots\n",
    "\n",
    "    return latbins, lonbins, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS TO SELECT THE DATA TO PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_of_predicted_categories(df):\n",
    "    \"\"\"Creates a list with all categories in the dataframe and a list with categories of the dataframe that have predictions.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "\n",
    "    Returns:\n",
    "        all_cats: A list with all categories in the dataframe, as they are (including the \"<hidden>\" labels that are found in some columns).\n",
    "        pred_cats: A list with categories that have predictions, as they are (including the \"<hidden>\" labels that are found in some columns).\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    all_cats = [c for c  in data.columns if \"-\" in c and \"prediction\" not in c] # This list contains all variable columns.\n",
    "    preds = [c for c in data.columns if \"prediction\" in c] # This list contains all columns that are predictions.\n",
    "    pred_cats = []\n",
    "    for c in all_cats:\n",
    "        for p in preds:\n",
    "            if c in p:\n",
    "                pred_cats.append(c)\n",
    "    return all_cats, pred_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(df, col, col_pred):\n",
    "    \"\"\"Creates a dataframe with a merged human/prediction column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "        col (str): Name of the column that contains the variable.\n",
    "        col_pred (str): Name of the column with the predicted variable.\n",
    "\n",
    "    Returns:\n",
    "        A modified dataframe with a column that has the predicted values if the paper was not seen by a human, and the \"human\" values if the paper was seen. The column has the same name of the variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = df.copy()\n",
    "    data['human'] = data[col] # This creates a copy of human data under the \"human\" header\n",
    "    data[col] = data[col_pred] # This replaces the column of interest (human data) with predicted data\n",
    "    \n",
    "    i = 0\n",
    "    while(i < len(data)):\n",
    "        if data['seen'].iloc[i] == 1:\n",
    "            data.at[i, col] = data['human'].iloc[i] # This replaces the predicted data, which is under the \"column of interest\" header, with the human data from the copied column.\n",
    "        i = i + 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(df, cat, prob=0.5):\n",
    "    \"\"\"Filters a dataframe based on a variable of interest.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "        cat (str): Name of the column that contains the variable. The name should be precise, including \"<hidden>\" labels. 'All' if no filtering is to be done.\n",
    "        prob (float): Minimum probability threshold to consider that a paper is about the variable of interest. Default: 0.5\n",
    "\n",
    "    Returns:\n",
    "        A dataframe that has been filtered according to a specific variable.\n",
    "    \"\"\"\n",
    "    filtered = df.copy()\n",
    "    all_cats, pred_cats = create_list_of_predicted_categories(df)\n",
    "    \n",
    "    if(cat == 'All' or cat == 'all'):\n",
    "        return filtered\n",
    "    elif(cat in pred_cats):\n",
    "        pred = cat + ' - prediction'\n",
    "        filtered = merge_columns(filtered, cat, pred)\n",
    "        filtered = filtered[filtered[cat] >= prob]\n",
    "        return filtered\n",
    "    elif(cat in all_cats):\n",
    "        filtered = filtered[filtered[cat] >= prob]\n",
    "        return filtered\n",
    "    else:\n",
    "        print('Wrong variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS TO PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(cat, degrees, distance, df, file_name, dpi=150, width=8, height=5):\n",
    "    \"\"\"Creates a map based on a variable of interest and saves it in a map.\n",
    "\n",
    "    Args:\n",
    "        cat (str): Variable of interest. Must be specific, including \"<hidden>\" labels. \"All\" if all papers are to be mapped.\n",
    "        degrees (float): Number of degrees.\n",
    "        distance (int): Distance of the clusters.\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "        file_name (str): Name of the plot file.\n",
    "        dpi (int): Resolution (dots per inch).\n",
    "        width (int): Width of the plot. Default: 8.\n",
    "        height (int): Height of the plot. Default: 5.\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(dpi=dpi, figsize=(width, height))\n",
    "    p = ccrs.Mollweide()\n",
    "    ax = plt.axes(projection=p)\n",
    "    ax.set_global()\n",
    "    ax.coastlines(lw=0.1)\n",
    "\n",
    "    filtered = filter(df, cat)\n",
    "    latbins, lonbins, n = density_grid(degrees,distance,filtered)\n",
    "\n",
    "    vm = n[~np.isnan(n)].max()\n",
    "    n[n == 0] = np.nan\n",
    "\n",
    "    pcm = plt.pcolormesh( \n",
    "        lonbins, latbins, n,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        norm=mpl.colors.LogNorm(vmin=1, vmax=vm),\n",
    "        alpha=0.5,\n",
    "        cmap=\"YlGnBu\"\n",
    "    )\n",
    "\n",
    "    fig.colorbar(pcm)\n",
    "    title = cat.split(\" - \")[1]\n",
    "    ax.set_title(title)\n",
    "    filename = 'plots/' + file_name + '.pdf'\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maps(df):\n",
    "    \"\"\"Plots all variables in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    all_cats, pred_cats = create_list_of_predicted_categories(df)\n",
    "    for counter, c in enumerate(all_cats):\n",
    "        file_name = c.split(' - ')[1].replace(\" \",\"_\").replace(\"/\", \"_\")\n",
    "        print(file_name)\n",
    "        plot_map(c, 1, 100, df, file_name)\n",
    "        print(str((counter/len(all_cats))*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_maps(data_geo)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593694235079",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}